{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "تمرین3",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+FXXoFFxMN3rjQeaU2imk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raziyebadpa/myclass_new/blob/main/%D8%AA%D9%85%D8%B1%DB%8C%D9%863.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CQVrBk-Rc6f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkaimNJfYZ2w"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcVIAVt8SRlP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFxztZQInlAB"
      },
      "source": [
        "# sport2 =\"tenis\"\n",
        "sport = tf.constant(\"Tennis\", tf.string)\n",
        "# sport = tf.constant(\"Tennisplayer\", tf.string)\n",
        "number = tf.constant(1.41421356237, tf.float64)\n",
        "# number = tf.constant(1.41421356237, tf.float32)\n",
        "\n",
        "print(\"`sport` is a {}-d Tensor\".format(tf.rank(sport).numpy()))\n",
        "# print(\"`sport is a {}-d Tensor\".format(tf.rank(sport).numpy()))\n",
        "# print(\"`sport` is a {}-d Tensor\".format(tf.shape(sport).numpy()))\n",
        "# print(\"`sport` is a {}-d Tensor\".format(tf.rank(sport))\n",
        "print(\"`number` is a {}-d Tensor\".format(tf.rank(number).numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZLJ14YKXztS"
      },
      "source": [
        "sports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\n",
        "numbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n",
        "# numbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float16)\n",
        "# \n",
        "\n",
        "print(\"`sports` is a {}-d Tensor with shape: {}\".format(tf.rank(sports).numpy(), tf.shape(sports)))\n",
        "# print(\"{} {}\".format(tf.rank(sports).numpy(), tf.shape(sports)))\n",
        "# print(\"{} {}\".format(tf.rank(sports), tf.shape(sports)))\n",
        "# print(\"aaa{} bbb{}\".format(tf.rank(sports).numpy(), tf.shape(sports)))\n",
        "\n",
        "print(\"`numbers` is a {}-d Tensor with shape: {}\".format(tf.rank(numbers).numpy(), tf.shape(numbers)))\n",
        "# print(\"`numbers` is a {}-d Tensor with shape: {}\".format(tf.rank(numbers), tf.shape(numbers)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5atE7vfEcj-k"
      },
      "source": [
        "### Defining higher-order Tensors ###\n",
        "\n",
        "'''TODO: Define a 2-d Tensor'''\n",
        "matrix = tf.constant([[3.141592, 1.414213, 2.71821],[1,1,1]], tf.float64)\n",
        "matrix = # TODO\n",
        "\n",
        "assert isinstance(matrix, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(matrix).numpy() == 2\n",
        "\n",
        "\n",
        "### '''TODO: Define a 3-d Tensor'''\n",
        "matrix = tf.constant([[[3.141592, 1.414213, 2.71821],[1,1,1]]], tf.float64)\n",
        "# matrix = tf.constant([[3.141592, 1.414213, 2.71821],[1,1,1]], tf.float64)\n",
        "assert isinstance(matrix, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(matrix).numpy() == 3\n",
        "print(\"`matrix` is a {}-d Tensor with shape: {}\".format(tf.rank(matrix).numpy(), tf.shape(matrix)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXUKP8aNf_9I"
      },
      "source": [
        "'''TODO: Define a 4-d Tensor.'''\n",
        "# Use tf.zeros to initialize a 4-d Tensor of zeros with size 10 x 256 x 256 x 3. \n",
        "#   You can think of this as 10 images where each image is RGB 256 x 256.\n",
        "images = tf.zeros([10,256,256,3])\n",
        "# images = tf.zeros([10,256,256,3], tf.int32)\n",
        "images = # TODO\n",
        "\n",
        "assert isinstance(images, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(images).numpy() == 4, \"matrix must be of rank 4\"\n",
        "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3], \"matrix is incorrect shape\"\n",
        "# print(\"`images` is a {}-d Tensor with shape: {}\".format(tf.rank(images).numpy(), tf.shape(images)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7fw4LIYhp5-"
      },
      "source": [
        "row_vector = matrix[1]\n",
        "column_vector = matrix[:,2]\n",
        "scalar = matrix[1, 2]\n",
        "\n",
        "# print(\"`row_vector`: {}\".format(matrix)\n",
        "print(\"`row_vector`: {}\".format(row_vector.numpy()))\n",
        "# print(\"`row_vector`: {}\".format(row_vector))\n",
        "# print(\"`row_vector`: {}\".format(matrix[1].numpy()))\n",
        "print(\"`column_vector`: {}\".format(column_vector.numpy()))\n",
        "# print(\"`column_vector`: {}\".format(matrix[:,2].numpy()))\n",
        "print(\"`scalar`: {}\".format(scalar.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wG1c8UGtdDB"
      },
      "source": [
        "# Create the nodes in the graph, and initialize values\n",
        "a = tf.constant(15)\n",
        "b = tf.constant(61)\n",
        "# c= tf.constant(42)\n",
        "\n",
        "# Add them!\n",
        "c1 = tf.add(a,b)\n",
        "# c1 = tf.sub(a,b)\n",
        "# c1 = tf.mul(a,b)\n",
        "# c1 = tf.add(a,b,c)\n",
        "# c1 = tf.mul(a,b,c)\n",
        "# c1 = tf.sub(a,b,c)\n",
        "c2 = a + b # TensorFlow overrides the \"+\" operation so that it is able to act on Tensors\n",
        "# c2 = a - b # TensorFlow overrides the \"-\" operation so that it is able to act on Tensors\n",
        "# c2 = a * b # TensorFlow overrides the \"*\" operation so that it is able to act on Tensors\n",
        "\n",
        "print(c1)\n",
        "# print(c1.numpy())\n",
        "print(c2)\n",
        "# print(c2.numpy())\n",
        "# assert tf.Tensor(76, shape=(), dtype=int32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs9bE2IkwnKY"
      },
      "source": [
        "### Defining Tensor computations ###\n",
        "\n",
        "# Construct a simple computation function\n",
        "def func(a,b):\n",
        "  '''TODO: Define the operation for c, d, e (use tf.add, tf.subtract, tf.multiply).'''\n",
        "  c = # TODO\n",
        "  d = # TODO\n",
        "  e = # TODO\n",
        "  return e\n",
        " ### def func(a,b):\n",
        "  '''TODO: Define the operation for c, d, e (use tf.add, tf.subtract, tf.multiply).'''\n",
        "  c = tf.add(a,b)\n",
        "  d = tf.subtract(b,1)\n",
        "  #  d = tf.subtract(b,2)\n",
        "\n",
        "  e = tf.multiply(c,d)\n",
        "  # e = tf.multiply(c,c,d)\n",
        "  return e\n",
        "  ### def func(a,b):\n",
        "  '''TODO: Define the operation for c, d, e (use tf.add, tf.div, tf.multiply).'''\n",
        "  c = tf.multiply(a,b)\n",
        "  # c_out = func(a,b)\n",
        "   print(c_out)\n",
        "  d = tf.add(b,1)\n",
        "  print(d_out)\n",
        "  e = tf.div(c,d)\n",
        "  print(e_out)\n",
        "  return e\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JrAla6YxMnt"
      },
      "source": [
        "a, b = 3.5, 0.5\n",
        "# Execute the computation\n",
        "e_out = func(a,b)\n",
        "print(e_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-9tYQmv7BfK"
      },
      "source": [
        "### Defining a network Layer ###\n",
        "\n",
        "# n_output_nodes: number of output nodes\n",
        "# input_shape: shape of the input\n",
        "# x: input to the layer\n",
        "\n",
        "class OurDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(OurDenseLayer, self).__init__()\n",
        "    self.n_output_nodes = n_output_nodes\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    d = int(input_shape[-1])\n",
        "    # Define and initialize parameters: a weight matrix W and bias b\n",
        "    # Note that parameter initialization is random!\n",
        "    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes]) # note the dimensionality\n",
        "    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes]) # note the dimensionality\n",
        "\n",
        "  def call(self, x):\n",
        "    '''TODO: define the operation for z (hint: use tf.matmul)'''\n",
        "    z = # TODO\n",
        "  #  z = tf.add(tf.matmul(x,self.W),self.b)\n",
        "  #  z = tf.add(tf.matmul(x,self.W),self.b+10)\n",
        "  #  z = tf.sub(tf.matmul(x,self.W),self.b)\n",
        "\n",
        "    '''TODO: define the operation for out (hint: use tf.sigmoid)'''\n",
        "    y = # TODO\n",
        "    #  y = tf.sigmoid(z)\n",
        "    # y=tf.tanh(z)\n",
        "    return y\n",
        "\n",
        "# Since layer parameters are initialized randomly, we will set a random seed for reproducibility\n",
        "tf.random.set_seed(1)\n",
        "layer = OurDenseLayer(3)\n",
        "layer.build((1,2))\n",
        "# layer.build((1,2,3))\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "# x_input = tf.constant([[1,2.]], shape=(1,2,3))\n",
        "# x_input = tf.constant([[1,2,3]], shape=(1,3))\n",
        "y = layer.call(x_input)\n",
        "\n",
        "# test the output!\n",
        "print(y.numpy())\n",
        "# print(y)\n",
        "mdl.lab1.test_custom_dense_layer_output(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz9Js66NAkDe"
      },
      "source": [
        "### Defining a neural network using the Sequential API ###\n",
        "\n",
        "# Import relevant packages\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define the number of outputs\n",
        "n_output_nodes = 3\n",
        "\n",
        "# First define the model \n",
        "model = Sequential()\n",
        "\n",
        "'''TODO: Define a dense (fully connected) layer to compute z'''\n",
        "# Remember: dense layers are defined by the parameters W and b!\n",
        "# You can read more about the initialization of W and b in the TF documentation :) \n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable\n",
        "dense_layer = # TODO\n",
        "# dense_layer = Dense(3,activation='sigmoid')\n",
        "# dense_layer = Dense(3,activation='tanh')\n",
        "\n",
        "model.add(dense_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBXsDPxqB0Mx"
      },
      "source": [
        "# Test model with example input\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "'''TODO: feed input into the model and predict the output!'''\n",
        "model_output = # TODO\n",
        "# model_output = model(x_input)\n",
        "print(model_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckAgL67jCQu1"
      },
      "source": [
        "### Defining a model using subclassing ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class SubclassModel(tf.keras.Model):\n",
        "\n",
        "  # In __init__, we define the Model's layers\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(SubclassModel, self).__init__()\n",
        "    '''TODO: Our model consists of a single Dense layer. Define this layer.''' \n",
        "    self.dense_layer = '''TODO: Dense Layer'''\n",
        "    # self.dense_layer =  Dense(n_output_nodes, activation='sigmoid')\n",
        "\n",
        "  # In the call function, we define the Model's forward pass.\n",
        "  def call(self, inputs):\n",
        "    return self.dense_layer(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beo21CZzJUzs"
      },
      "source": [
        "n_output_nodes = 3\n",
        "model = SubclassModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "print(model.call(x_input))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKCGh8grJwdL"
      },
      "source": [
        "### Defining a model using subclassing and specifying custom behavior ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class IdentityModel(tf.keras.Model):\n",
        "\n",
        "  # As before, in __init__ we define the Model's layers\n",
        "  # Since our desired behavior involves the forward pass, this part is unchanged\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(IdentityModel, self).__init__()\n",
        "    self.dense_layer = tf.keras.layers.Dense(n_output_nodes, activation='sigmoid')\n",
        "\n",
        "  '''TODO: Implement the behavior where the network outputs the input, unchanged, \n",
        "      under control of the isidentity argument.'''\n",
        "  def call(self, inputs, isidentity=False):\n",
        "    x = self.dense_layer(inputs)\n",
        "    # x = self.dense_layer(inputs)\n",
        "    if isidentity: # TODO\n",
        "      return inputs # TODO\n",
        "    return x\n",
        "    '''TODO: Implement identity behavior'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC_MUuewJzyd"
      },
      "source": [
        "n_output_nodes = 3\n",
        "model = IdentityModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "'''TODO: pass the input into the model and call with and without the input identity option.'''\n",
        "# print(x_input)\n",
        "out_activate = # TODO\n",
        "# out_activate = model(x_input)\n",
        "out_identity = # TODO\n",
        "# out_identity = model(x_input,isidentity=True)\n",
        "\n",
        "print(\"Network output with activation: {}; network identity output: {}\".format(out_activate.numpy(), out_identity.numpy()))\n",
        "# print(\"Network output with activation: {}; network identity output: {}\".format(model(x_input), out_identity.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXpMr2ssLEWD"
      },
      "source": [
        "### Gradient computation with GradientTape ###\n",
        "\n",
        "# y = x^2\n",
        "# Example: x = 3.0\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "# Initiate the gradient tape\n",
        "with tf.GradientTape() as tape:\n",
        "  # Define the function\n",
        "  y = x * x\n",
        "  # y = x * x* x\n",
        "  # y = 2 * x \n",
        "# Access the gradient -- derivative of y with respect to x\n",
        "dy_dx = tape.gradient(y, x)\n",
        "\n",
        "assert dy_dx.numpy() == 27.0\n",
        "# assert dy_dx.numpy() == 2.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRvrtN4zMegH"
      },
      "source": [
        "### Function minimization with automatic differentiation and SGD ###\n",
        "\n",
        "# Initialize a random value for our initial x\n",
        "x = tf.Variable([tf.random.normal([1])])\n",
        "print(\"Initializing x={}\".format(x.numpy()))\n",
        "\n",
        "learning_rate = 1e-2 # learning rate for SGD\n",
        "history = []\n",
        "# Define the target value\n",
        "x_f = 4\n",
        "\n",
        "# We will run SGD for a number of iterations. At each iteration, we compute the loss, \n",
        "#   compute the derivative of the loss with respect to x, and perform the SGD update.\n",
        "for i in range(500):\n",
        "  with tf.GradientTape() as tape:\n",
        "    '''TODO: define the loss as described above'''\n",
        "    loss = # TODO\n",
        "#  loss = tf.square(tf.subtract(x,x_f)\n",
        "  # loss minimization using gradient tape\n",
        "  grad = tape.gradient(loss, x) # compute the derivative of the loss with respect to x\n",
        "  new_x = x - learning_rate*grad # sgd update\n",
        "  x.assign(new_x) # update the value of x\n",
        "  history.append(x.numpy()[0])\n",
        "\n",
        "# Plot the evolution of x as we optimize towards x_f!\n",
        "plt.plot(history)\n",
        "plt.plot([0, 500],[x_f,x_f])\n",
        "# plt.plot([10, 500],[x_f,x_f])\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')\n",
        "# Out[ ]:Text(0, 0.5, 'x value')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}